{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Credit Card Fraud\n",
    "\n",
    "In my current position at SmartBiz, I am the data sherpa, and that means I'm at the intersection of all business units and their data needs. I've been fortunate to have opportunities to work with our Data Science team and ultimately found that this area is one where I really want to move forward in.\n",
    "\n",
    "One of the major projects I collaborated with the Data Science team on was predictive modeling of the performance of loans. This was a \n",
    "\n",
    "## Goal\n",
    "- Produce a predictive model as evaluated by Area Under the Precision-Recall Curve and Average Precision Score.\n",
    "\n",
    "## Process\n",
    "- Explore Data\n",
    "- Slice a few different ways and compare model performance between the methods:\n",
    "    - Raw features\n",
    "    - Features with high variance between classes (`high_var_feats`)\n",
    "    - Using `high_var_features`, perform DFS to generate more features (`dfs_feats`)\n",
    "    - Using `dfs_features`, perform CV RFE to determine optimal number of features and which features to keep (`rfe_feats`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data\n",
    "Let's go through the basics and explore this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_means = pd.concat([\n",
    "    data[data['Class'] == 0].mean().round(3),\n",
    "    data[data['Class'] == 1].mean().round(3)\n",
    "], axis = 1)\n",
    "\n",
    "compare_means.columns = ['Legit','Fraud']\n",
    "\n",
    "compare_means['AbsVar'] = abs(compare_means['Fraud'] - compare_means['Legit'])\n",
    "\n",
    "compare_means['AbsPct'] = (compare_means['AbsVar'] / compare_means['Legit']).round(3) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Legit</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>AbsVar</th>\n",
       "      <th>AbsPct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>94838.202</td>\n",
       "      <td>80746.807</td>\n",
       "      <td>14091.395</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>88.291</td>\n",
       "      <td>122.211</td>\n",
       "      <td>33.920</td>\n",
       "      <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-7.033</td>\n",
       "      <td>7.045</td>\n",
       "      <td>58708.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-6.972</td>\n",
       "      <td>6.984</td>\n",
       "      <td>58200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-6.666</td>\n",
       "      <td>6.678</td>\n",
       "      <td>55650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-6.259</td>\n",
       "      <td>6.270</td>\n",
       "      <td>57000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.677</td>\n",
       "      <td>5.687</td>\n",
       "      <td>56870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.569</td>\n",
       "      <td>5.579</td>\n",
       "      <td>55790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>0.008</td>\n",
       "      <td>-4.772</td>\n",
       "      <td>4.780</td>\n",
       "      <td>59750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>4.542</td>\n",
       "      <td>4.550</td>\n",
       "      <td>-56875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.007</td>\n",
       "      <td>-4.140</td>\n",
       "      <td>4.147</td>\n",
       "      <td>59242.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>-54385.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>3.624</td>\n",
       "      <td>3.630</td>\n",
       "      <td>-60500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-3.151</td>\n",
       "      <td>3.156</td>\n",
       "      <td>63120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-2.581</td>\n",
       "      <td>2.585</td>\n",
       "      <td>64625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>2.250</td>\n",
       "      <td>56250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.398</td>\n",
       "      <td>1.400</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-71500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-68200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-57200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-37300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.109</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.105</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.093</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Legit      Fraud     AbsVar   AbsPct\n",
       "Time    94838.202  80746.807  14091.395     14.9\n",
       "Amount     88.291    122.211     33.920     38.4\n",
       "V3          0.012     -7.033      7.045  58708.3\n",
       "V14         0.012     -6.972      6.984  58200.0\n",
       "V17         0.012     -6.666      6.678  55650.0\n",
       "V12         0.011     -6.259      6.270  57000.0\n",
       "V10         0.010     -5.677      5.687  56870.0\n",
       "V7          0.010     -5.569      5.579  55790.0\n",
       "V1          0.008     -4.772      4.780  59750.0\n",
       "V4         -0.008      4.542      4.550 -56875.0\n",
       "V16         0.007     -4.140      4.147  59242.9\n",
       "V11        -0.007      3.800      3.807 -54385.7\n",
       "V2         -0.006      3.624      3.630 -60500.0\n",
       "V5          0.005     -3.151      3.156  63120.0\n",
       "V9          0.004     -2.581      2.585  64625.0\n",
       "V18         0.004     -2.246      2.250  56250.0\n",
       "V6          0.002     -1.398      1.400  70000.0\n",
       "Class       0.000      1.000      1.000      inf\n",
       "V21        -0.001      0.714      0.715 -71500.0\n",
       "V19        -0.001      0.681      0.682 -68200.0\n",
       "V8         -0.001      0.571      0.572 -57200.0\n",
       "V20        -0.001      0.372      0.373 -37300.0\n",
       "V27        -0.000      0.171      0.171     -inf\n",
       "V13         0.000     -0.109      0.109      inf\n",
       "V24         0.000     -0.105      0.105      inf\n",
       "V15         0.000     -0.093      0.093      inf\n",
       "V28        -0.000      0.076      0.076     -inf\n",
       "V26        -0.000      0.052      0.052     -inf\n",
       "V25        -0.000      0.041      0.041     -inf\n",
       "V23         0.000     -0.040      0.040      inf\n",
       "V22        -0.000      0.014      0.014     -inf"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_means.sort_values(by='AbsVar', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_std = pd.concat([\n",
    "    data[data['Class'] == 0].std().round(3),\n",
    "    data[data['Class'] == 1].std().round(3)\n",
    "], axis = 1)\n",
    "\n",
    "compare_std.columns = ['Legit','Fraud']\n",
    "\n",
    "compare_std['Var'] = compare_std['Fraud'] - compare_std['Legit']\n",
    "\n",
    "compare_std['AbsPct'] = (compare_std['Var'] / compare_std['Legit']).round(3) * 100\n",
    "\n",
    "compare_std.columns = ['{}_std'.format(i) for i in compare_std.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Legit_std</th>\n",
       "      <th>Fraud_std</th>\n",
       "      <th>Var_std</th>\n",
       "      <th>AbsPct_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.749</td>\n",
       "      <td>6.971</td>\n",
       "      <td>6.222</td>\n",
       "      <td>830.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>1.179</td>\n",
       "      <td>7.207</td>\n",
       "      <td>6.028</td>\n",
       "      <td>511.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>1.161</td>\n",
       "      <td>6.798</td>\n",
       "      <td>5.637</td>\n",
       "      <td>485.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>0.717</td>\n",
       "      <td>3.869</td>\n",
       "      <td>3.152</td>\n",
       "      <td>439.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.946</td>\n",
       "      <td>4.654</td>\n",
       "      <td>3.708</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>1.459</td>\n",
       "      <td>7.111</td>\n",
       "      <td>5.652</td>\n",
       "      <td>387.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>0.897</td>\n",
       "      <td>4.279</td>\n",
       "      <td>3.382</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>1.044</td>\n",
       "      <td>4.897</td>\n",
       "      <td>3.853</td>\n",
       "      <td>369.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.845</td>\n",
       "      <td>3.865</td>\n",
       "      <td>3.020</td>\n",
       "      <td>357.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>1.357</td>\n",
       "      <td>5.372</td>\n",
       "      <td>4.015</td>\n",
       "      <td>295.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>1.930</td>\n",
       "      <td>6.784</td>\n",
       "      <td>4.854</td>\n",
       "      <td>251.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.825</td>\n",
       "      <td>2.899</td>\n",
       "      <td>2.074</td>\n",
       "      <td>251.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>0.400</td>\n",
       "      <td>1.377</td>\n",
       "      <td>0.977</td>\n",
       "      <td>244.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>1.003</td>\n",
       "      <td>2.679</td>\n",
       "      <td>1.676</td>\n",
       "      <td>167.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>1.636</td>\n",
       "      <td>4.291</td>\n",
       "      <td>2.655</td>\n",
       "      <td>162.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>0.622</td>\n",
       "      <td>1.580</td>\n",
       "      <td>0.958</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>1.089</td>\n",
       "      <td>2.501</td>\n",
       "      <td>1.412</td>\n",
       "      <td>129.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>0.724</td>\n",
       "      <td>1.495</td>\n",
       "      <td>0.771</td>\n",
       "      <td>106.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>1.399</td>\n",
       "      <td>2.873</td>\n",
       "      <td>1.474</td>\n",
       "      <td>105.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>0.812</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.728</td>\n",
       "      <td>89.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>0.769</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.578</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.217</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>0.521</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.276</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>1.330</td>\n",
       "      <td>1.858</td>\n",
       "      <td>0.528</td>\n",
       "      <td>39.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.915</td>\n",
       "      <td>1.050</td>\n",
       "      <td>0.135</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.995</td>\n",
       "      <td>1.105</td>\n",
       "      <td>0.110</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>250.105</td>\n",
       "      <td>256.683</td>\n",
       "      <td>6.578</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>47484.016</td>\n",
       "      <td>47835.365</td>\n",
       "      <td>351.349</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Legit_std  Fraud_std  Var_std  AbsPct_std\n",
       "V17         0.749      6.971    6.222       830.7\n",
       "V7          1.179      7.207    6.028       511.3\n",
       "V8          1.161      6.798    5.637       485.5\n",
       "V21         0.717      3.869    3.152       439.6\n",
       "V12         0.946      4.654    3.708       392.0\n",
       "V3          1.459      7.111    5.652       387.4\n",
       "V14         0.897      4.279    3.382       377.0\n",
       "V10         1.044      4.897    3.853       369.1\n",
       "V16         0.845      3.865    3.020       357.4\n",
       "V5          1.357      5.372    4.015       295.9\n",
       "V1          1.930      6.784    4.854       251.5\n",
       "V18         0.825      2.899    2.074       251.4\n",
       "V27         0.400      1.377    0.977       244.2\n",
       "V11         1.003      2.679    1.676       167.1\n",
       "V2          1.636      4.291    2.655       162.3\n",
       "V23         0.622      1.580    0.958       154.0\n",
       "V9          1.089      2.501    1.412       129.7\n",
       "V22         0.724      1.495    0.771       106.5\n",
       "V4          1.399      2.873    1.474       105.4\n",
       "V19         0.812      1.540    0.728        89.7\n",
       "V20         0.769      1.347    0.578        75.2\n",
       "V28         0.330      0.547    0.217        65.8\n",
       "V25         0.521      0.797    0.276        53.0\n",
       "V6          1.330      1.858    0.528        39.7\n",
       "V15         0.915      1.050    0.135        14.8\n",
       "V13         0.995      1.105    0.110        11.1\n",
       "Amount    250.105    256.683    6.578         2.6\n",
       "Time    47484.016  47835.365  351.349         0.7\n",
       "V26         0.482      0.472   -0.010        -2.1\n",
       "V24         0.606      0.516   -0.090       -14.9\n",
       "Class       0.000      0.000    0.000         NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_std.sort_values(by='AbsPct_std', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mean_and_std = pd.concat([\n",
    "    compare_means,\n",
    "    compare_std\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mean_and_std['quotient'] = compare_mean_and_std['AbsVar'] / compare_mean_and_std['Var_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Legit</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>AbsVar</th>\n",
       "      <th>AbsPct</th>\n",
       "      <th>Legit_std</th>\n",
       "      <th>Fraud_std</th>\n",
       "      <th>Var_std</th>\n",
       "      <th>AbsPct_std</th>\n",
       "      <th>quotient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>94838.202</td>\n",
       "      <td>80746.807</td>\n",
       "      <td>14091.395</td>\n",
       "      <td>14.9</td>\n",
       "      <td>47484.016</td>\n",
       "      <td>47835.365</td>\n",
       "      <td>351.349</td>\n",
       "      <td>0.7</td>\n",
       "      <td>40.106546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>88.291</td>\n",
       "      <td>122.211</td>\n",
       "      <td>33.920</td>\n",
       "      <td>38.4</td>\n",
       "      <td>250.105</td>\n",
       "      <td>256.683</td>\n",
       "      <td>6.578</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.156583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>4.542</td>\n",
       "      <td>4.550</td>\n",
       "      <td>-56875.0</td>\n",
       "      <td>1.399</td>\n",
       "      <td>2.873</td>\n",
       "      <td>1.474</td>\n",
       "      <td>105.4</td>\n",
       "      <td>3.086839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.398</td>\n",
       "      <td>1.400</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.858</td>\n",
       "      <td>0.528</td>\n",
       "      <td>39.7</td>\n",
       "      <td>2.651515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>-54385.7</td>\n",
       "      <td>1.003</td>\n",
       "      <td>2.679</td>\n",
       "      <td>1.676</td>\n",
       "      <td>167.1</td>\n",
       "      <td>2.271480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-6.972</td>\n",
       "      <td>6.984</td>\n",
       "      <td>58200.0</td>\n",
       "      <td>0.897</td>\n",
       "      <td>4.279</td>\n",
       "      <td>3.382</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.065050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-2.581</td>\n",
       "      <td>2.585</td>\n",
       "      <td>64625.0</td>\n",
       "      <td>1.089</td>\n",
       "      <td>2.501</td>\n",
       "      <td>1.412</td>\n",
       "      <td>129.7</td>\n",
       "      <td>1.830737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-6.259</td>\n",
       "      <td>6.270</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0.946</td>\n",
       "      <td>4.654</td>\n",
       "      <td>3.708</td>\n",
       "      <td>392.0</td>\n",
       "      <td>1.690939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.677</td>\n",
       "      <td>5.687</td>\n",
       "      <td>56870.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>4.897</td>\n",
       "      <td>3.853</td>\n",
       "      <td>369.1</td>\n",
       "      <td>1.475993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.007</td>\n",
       "      <td>-4.140</td>\n",
       "      <td>4.147</td>\n",
       "      <td>59242.9</td>\n",
       "      <td>0.845</td>\n",
       "      <td>3.865</td>\n",
       "      <td>3.020</td>\n",
       "      <td>357.4</td>\n",
       "      <td>1.373179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>3.624</td>\n",
       "      <td>3.630</td>\n",
       "      <td>-60500.0</td>\n",
       "      <td>1.636</td>\n",
       "      <td>4.291</td>\n",
       "      <td>2.655</td>\n",
       "      <td>162.3</td>\n",
       "      <td>1.367232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-7.033</td>\n",
       "      <td>7.045</td>\n",
       "      <td>58708.3</td>\n",
       "      <td>1.459</td>\n",
       "      <td>7.111</td>\n",
       "      <td>5.652</td>\n",
       "      <td>387.4</td>\n",
       "      <td>1.246461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>2.250</td>\n",
       "      <td>56250.0</td>\n",
       "      <td>0.825</td>\n",
       "      <td>2.899</td>\n",
       "      <td>2.074</td>\n",
       "      <td>251.4</td>\n",
       "      <td>1.084860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-6.666</td>\n",
       "      <td>6.678</td>\n",
       "      <td>55650.0</td>\n",
       "      <td>0.749</td>\n",
       "      <td>6.971</td>\n",
       "      <td>6.222</td>\n",
       "      <td>830.7</td>\n",
       "      <td>1.073288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.109</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.105</td>\n",
       "      <td>0.110</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.990909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>0.008</td>\n",
       "      <td>-4.772</td>\n",
       "      <td>4.780</td>\n",
       "      <td>59750.0</td>\n",
       "      <td>1.930</td>\n",
       "      <td>6.784</td>\n",
       "      <td>4.854</td>\n",
       "      <td>251.5</td>\n",
       "      <td>0.984755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-68200.0</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.728</td>\n",
       "      <td>89.7</td>\n",
       "      <td>0.936813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.569</td>\n",
       "      <td>5.579</td>\n",
       "      <td>55790.0</td>\n",
       "      <td>1.179</td>\n",
       "      <td>7.207</td>\n",
       "      <td>6.028</td>\n",
       "      <td>511.3</td>\n",
       "      <td>0.925514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-3.151</td>\n",
       "      <td>3.156</td>\n",
       "      <td>63120.0</td>\n",
       "      <td>1.357</td>\n",
       "      <td>5.372</td>\n",
       "      <td>4.015</td>\n",
       "      <td>295.9</td>\n",
       "      <td>0.786052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Legit      Fraud     AbsVar   AbsPct  Legit_std  Fraud_std  \\\n",
       "Class       0.000      1.000      1.000      inf      0.000      0.000   \n",
       "Time    94838.202  80746.807  14091.395     14.9  47484.016  47835.365   \n",
       "Amount     88.291    122.211     33.920     38.4    250.105    256.683   \n",
       "V4         -0.008      4.542      4.550 -56875.0      1.399      2.873   \n",
       "V6          0.002     -1.398      1.400  70000.0      1.330      1.858   \n",
       "V11        -0.007      3.800      3.807 -54385.7      1.003      2.679   \n",
       "V14         0.012     -6.972      6.984  58200.0      0.897      4.279   \n",
       "V9          0.004     -2.581      2.585  64625.0      1.089      2.501   \n",
       "V12         0.011     -6.259      6.270  57000.0      0.946      4.654   \n",
       "V10         0.010     -5.677      5.687  56870.0      1.044      4.897   \n",
       "V16         0.007     -4.140      4.147  59242.9      0.845      3.865   \n",
       "V2         -0.006      3.624      3.630 -60500.0      1.636      4.291   \n",
       "V3          0.012     -7.033      7.045  58708.3      1.459      7.111   \n",
       "V18         0.004     -2.246      2.250  56250.0      0.825      2.899   \n",
       "V17         0.012     -6.666      6.678  55650.0      0.749      6.971   \n",
       "V13         0.000     -0.109      0.109      inf      0.995      1.105   \n",
       "V1          0.008     -4.772      4.780  59750.0      1.930      6.784   \n",
       "V19        -0.001      0.681      0.682 -68200.0      0.812      1.540   \n",
       "V7          0.010     -5.569      5.579  55790.0      1.179      7.207   \n",
       "V5          0.005     -3.151      3.156  63120.0      1.357      5.372   \n",
       "\n",
       "        Var_std  AbsPct_std   quotient  \n",
       "Class     0.000         NaN        inf  \n",
       "Time    351.349         0.7  40.106546  \n",
       "Amount    6.578         2.6   5.156583  \n",
       "V4        1.474       105.4   3.086839  \n",
       "V6        0.528        39.7   2.651515  \n",
       "V11       1.676       167.1   2.271480  \n",
       "V14       3.382       377.0   2.065050  \n",
       "V9        1.412       129.7   1.830737  \n",
       "V12       3.708       392.0   1.690939  \n",
       "V10       3.853       369.1   1.475993  \n",
       "V16       3.020       357.4   1.373179  \n",
       "V2        2.655       162.3   1.367232  \n",
       "V3        5.652       387.4   1.246461  \n",
       "V18       2.074       251.4   1.084860  \n",
       "V17       6.222       830.7   1.073288  \n",
       "V13       0.110        11.1   0.990909  \n",
       "V1        4.854       251.5   0.984755  \n",
       "V19       0.728        89.7   0.936813  \n",
       "V7        6.028       511.3   0.925514  \n",
       "V5        4.015       295.9   0.786052  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mean_and_std[\n",
    "    compare_mean_and_std['quotient'] > 0.75\n",
    "].sort_values(by='quotient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_var_feats = compare_mean_and_std[\n",
    "    compare_mean_and_std['quotient'] > 0.75\n",
    "].sort_values(by='quotient', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Class', 'Time', 'Amount', 'V4', 'V6', 'V11', 'V14', 'V9', 'V12', 'V10',\n",
       "       'V16', 'V2', 'V3', 'V18', 'V17', 'V13', 'V1', 'V19', 'V7', 'V5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_var_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b361cf0dd8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(data[high_var_feats].corr(),cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of the Classes\n",
    "from MyDataTools import explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Var Feats\n",
    "data_high_var = data[high_var_feats]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "e.compare_hists(df=data_high_var, target_col='Class', specific_cols=None, cap_floor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Check\n",
    "- Null values?\n",
    "- Normalizing?\n",
    "- Incorrect values?\n",
    "- Possibility of feature engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Class.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# S = StandardScaler()\n",
    "\n",
    "# data['Time_scaled'] = S.fit_transform(X=data[['Time']])\n",
    "# data['Amount_scaled'] = S.fit_transform(X=data[['Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like everything's there. That makes things easy. Let's get weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "## Bonus packages\n",
    "# from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw values first\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data.Class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_cap = pd.DataFrame(index=X.index)\n",
    "for i in X.columns:\n",
    "    X_cap[i] = e.set_cap_floor(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataTools import model\n",
    "\n",
    "m = model()\n",
    "\n",
    "# bsmote = BorderlineSMOTE()\n",
    "\n",
    "# rus = RandomUnderSampler()\n",
    "\n",
    "rf0 = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# pipe_rf_os = Pipeline(steps=[\n",
    "#     ('BSMOTE',bsmote),\n",
    "#     ('rf0',rf0)\n",
    "# ])\n",
    "\n",
    "# pipe_rf_us = Pipeline(steps=[\n",
    "#     ('RUS',rus),\n",
    "#     ('rf0',rf0)\n",
    "# ])\n",
    "\n",
    "gb0 = GradientBoostingClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "m.cross_val_roc_auc_plot([rf0, gb0, pipe_rf_os, pipe_rf_us, xgb], xval = X_val, yval = y_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "m.cross_val_precision_recall_auc([rf0, gb0, pipe_rf_os, pipe_rf_us, xgb], xval = X_val, yval = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.73      0.82        98\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56962\n",
      "   macro avg       0.97      0.87      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56859     5]\n",
      " [   26    72]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = xgb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8036637536999544"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(y_test, probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7346938775510204"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = m.optimal_cutoff(y_test, probas)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006662164232693613\n"
     ]
    }
   ],
   "source": [
    "print(thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_thres = [x > thres for x in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56864\n",
      "           1       0.02      0.93      0.05        98\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56962\n",
      "   macro avg       0.51      0.93      0.51     56962\n",
      "weighted avg       1.00      0.94      0.96     56962\n",
      "\n",
      "\n",
      "\n",
      "[[53187  3677]\n",
      " [    7    91]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_thres))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935064935064935"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv, DMatrix\n",
    "from sklearn.metrics import average_precision_score, make_scorer\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "avg_prec = make_scorer(average_precision_score, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Integer(low=3, high=10)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Integer(3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Categorical, Integer\n",
    "from xgboost import DMatrix, cv\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = DMatrix(X_train.values, label=y_train.values)\n",
    "cvresult = cv(xgb_param, xgtrain, num_boost_round=xgb.get_params()['n_estimators'], nfold=5,\n",
    "    metrics='auc', early_stopping_rounds=50)\n",
    "xgb.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913789</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.903821</td>\n",
       "      <td>0.027830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.920668</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.913805</td>\n",
       "      <td>0.009822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.926930</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.920346</td>\n",
       "      <td>0.016389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.926936</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.920345</td>\n",
       "      <td>0.016385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.927746</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.920351</td>\n",
       "      <td>0.016390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.927751</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.921663</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.929309</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.923356</td>\n",
       "      <td>0.019138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.930119</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.924803</td>\n",
       "      <td>0.018553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930124</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.924805</td>\n",
       "      <td>0.018557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.930128</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.924802</td>\n",
       "      <td>0.018568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.930130</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.924804</td>\n",
       "      <td>0.018561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.930133</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.924804</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.931306</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.924803</td>\n",
       "      <td>0.018550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.931310</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.924805</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.932061</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.930793</td>\n",
       "      <td>0.013721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.932064</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.930799</td>\n",
       "      <td>0.013722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.932064</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.930802</td>\n",
       "      <td>0.013722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.932469</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.932466</td>\n",
       "      <td>0.013396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.932848</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.932469</td>\n",
       "      <td>0.013396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.933237</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.932474</td>\n",
       "      <td>0.013392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.933234</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.932474</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.934448</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.932472</td>\n",
       "      <td>0.013401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.932474</td>\n",
       "      <td>0.013402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.934838</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.932475</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.937107</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.932428</td>\n",
       "      <td>0.013403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.939775</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.932384</td>\n",
       "      <td>0.013393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.941361</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.933642</td>\n",
       "      <td>0.012612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.933643</td>\n",
       "      <td>0.012612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.942137</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.933632</td>\n",
       "      <td>0.012612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.942138</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.933632</td>\n",
       "      <td>0.012613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.993770</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.975438</td>\n",
       "      <td>0.015903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.994153</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.976093</td>\n",
       "      <td>0.016542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.994428</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.016495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.994832</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.976025</td>\n",
       "      <td>0.016326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.995091</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.977133</td>\n",
       "      <td>0.014623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.995284</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.976899</td>\n",
       "      <td>0.014555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.995449</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.978379</td>\n",
       "      <td>0.012962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.995660</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.978717</td>\n",
       "      <td>0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.996044</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.978898</td>\n",
       "      <td>0.013568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.978796</td>\n",
       "      <td>0.013610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.996605</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.980599</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.996990</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.980618</td>\n",
       "      <td>0.013371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.997073</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.980750</td>\n",
       "      <td>0.013788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.997223</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.981249</td>\n",
       "      <td>0.013499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.997279</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.981508</td>\n",
       "      <td>0.013725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.997340</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.981729</td>\n",
       "      <td>0.013465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.997422</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.981489</td>\n",
       "      <td>0.013004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.997516</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.982899</td>\n",
       "      <td>0.012449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.997660</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.982879</td>\n",
       "      <td>0.012326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.997826</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.982832</td>\n",
       "      <td>0.012493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.997913</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.983179</td>\n",
       "      <td>0.012743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.997942</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.012565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.997998</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.983801</td>\n",
       "      <td>0.012357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.998073</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.983622</td>\n",
       "      <td>0.012625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.998090</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.983668</td>\n",
       "      <td>0.012601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.998175</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.984246</td>\n",
       "      <td>0.013005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.998224</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.984422</td>\n",
       "      <td>0.012892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.998257</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.984554</td>\n",
       "      <td>0.012269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.984714</td>\n",
       "      <td>0.012071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.998393</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.984630</td>\n",
       "      <td>0.012271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0         0.913789       0.016999       0.903821      0.027830\n",
       "1         0.920668       0.017108       0.913805      0.009822\n",
       "2         0.926930       0.005841       0.920346      0.016389\n",
       "3         0.926936       0.005846       0.920345      0.016385\n",
       "4         0.927746       0.005589       0.920351      0.016390\n",
       "5         0.927751       0.005590       0.921663      0.016200\n",
       "6         0.929309       0.003171       0.923356      0.019138\n",
       "7         0.930119       0.003873       0.924803      0.018553\n",
       "8         0.930124       0.003869       0.924805      0.018557\n",
       "9         0.930128       0.003866       0.924802      0.018568\n",
       "10        0.930130       0.003871       0.924804      0.018561\n",
       "11        0.930133       0.003874       0.924804      0.018555\n",
       "12        0.931306       0.003733       0.924803      0.018550\n",
       "13        0.931310       0.003728       0.924805      0.018548\n",
       "14        0.932061       0.003391       0.930793      0.013721\n",
       "15        0.932064       0.003392       0.930799      0.013722\n",
       "16        0.932064       0.003391       0.930802      0.013722\n",
       "17        0.932469       0.003794       0.932466      0.013396\n",
       "18        0.932848       0.003817       0.932469      0.013396\n",
       "19        0.933237       0.003092       0.932474      0.013392\n",
       "20        0.933234       0.003087       0.932474      0.013398\n",
       "21        0.934448       0.003828       0.932472      0.013401\n",
       "22        0.934449       0.003828       0.932474      0.013402\n",
       "23        0.934838       0.003151       0.932475      0.013400\n",
       "24        0.937107       0.004010       0.932428      0.013403\n",
       "25        0.939775       0.003418       0.932384      0.013393\n",
       "26        0.941361       0.002573       0.933642      0.012612\n",
       "27        0.941364       0.002572       0.933643      0.012612\n",
       "28        0.942137       0.003362       0.933632      0.012612\n",
       "29        0.942138       0.003361       0.933632      0.012613\n",
       "..             ...            ...            ...           ...\n",
       "70        0.993770       0.001540       0.975438      0.015903\n",
       "71        0.994153       0.001550       0.976093      0.016542\n",
       "72        0.994428       0.001531       0.976103      0.016495\n",
       "73        0.994832       0.001402       0.976025      0.016326\n",
       "74        0.995091       0.001489       0.977133      0.014623\n",
       "75        0.995284       0.001466       0.976899      0.014555\n",
       "76        0.995449       0.001642       0.978379      0.012962\n",
       "77        0.995660       0.001682       0.978717      0.013493\n",
       "78        0.996044       0.001391       0.978898      0.013568\n",
       "79        0.996235       0.001301       0.978796      0.013610\n",
       "80        0.996605       0.001359       0.980599      0.013800\n",
       "81        0.996990       0.001015       0.980618      0.013371\n",
       "82        0.997073       0.001085       0.980750      0.013788\n",
       "83        0.997223       0.001117       0.981249      0.013499\n",
       "84        0.997279       0.001103       0.981508      0.013725\n",
       "85        0.997340       0.001108       0.981729      0.013465\n",
       "86        0.997422       0.001073       0.981489      0.013004\n",
       "87        0.997516       0.001023       0.982899      0.012449\n",
       "88        0.997660       0.000948       0.982879      0.012326\n",
       "89        0.997826       0.000810       0.982832      0.012493\n",
       "90        0.997913       0.000748       0.983179      0.012743\n",
       "91        0.997942       0.000753       0.983491      0.012565\n",
       "92        0.997998       0.000763       0.983801      0.012357\n",
       "93        0.998073       0.000701       0.983622      0.012625\n",
       "94        0.998090       0.000678       0.983668      0.012601\n",
       "95        0.998175       0.000608       0.984246      0.013005\n",
       "96        0.998224       0.000650       0.984422      0.012892\n",
       "97        0.998257       0.000663       0.984554      0.012269\n",
       "98        0.998300       0.000625       0.984714      0.012071\n",
       "99        0.998393       0.000590       0.984630      0.012271\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=False, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = DMatrix(X_train.values, label=y_train.values)\n",
    "        cvresult = cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics=avg_prec, early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train, eval_metric=avg_prec)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X_train)\n",
    "    dtrain_predprob = alg.predict_proba(X_train)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(y_train, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(y_train, dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9997\n",
      "AUC Score (Train): 0.997327\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-9d0f50c58ab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-121-43183d9132dd>\u001b[0m in \u001b[0;36mmodelfit\u001b[1;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mfeat_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Feature Importances'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature Importance Score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "modelfit(xgb, dtrain=None, predictors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_space = {\n",
    "    'max_depth':Integer(3,10),\n",
    "    'min_child_weight':Integer(1,6),\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "     'gamma':[i/10.0 for i in range(0,5)],\n",
    "     'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=True, n_iter=5, n_jobs=1, n_points=1,\n",
       "       optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "       refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "       search_spaces={'max_depth': Integer(low=3, high=10), 'min_child_weight': Integer(low=1, high=6), 'subsample': [0.6, 0.7, 0.8, 0.9], 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]},\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo1 = BayesSearchCV(estimator=xgb, search_spaces= xgb_space,\n",
    "                      scoring=avg_prec,\n",
    "                      n_iter=5)\n",
    "\n",
    "\n",
    "xgb_bo1.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7704724465011741"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.7, gamma=0.1,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo1.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bo1 = xgb_bo1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.95      0.78      0.85        98\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56962\n",
      "   macro avg       0.97      0.89      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "\n",
      "[[56860     4]\n",
      " [   22    76]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_bo1))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_bo1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres_bo1 = m.optimal_cutoff(y_test, xgb_bo1.best_estimator_.predict_proba(X_test)[:,1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00042143784230574965\n"
     ]
    }
   ],
   "source": [
    "print(thres_bo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_new = [x > thres_bo1 for x in xgb_bo1.best_estimator_.predict_proba(X_test)[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56864\n",
      "           1       0.03      0.94      0.05        98\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56962\n",
      "   macro avg       0.51      0.94      0.51     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53329  3535]\n",
      " [    6    92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=2, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=True, n_iter=5, n_jobs=1, n_points=1,\n",
       "       optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "       refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "       search_spaces={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]}, verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb_bo2 = BayesSearchCV(estimator=xgb_bo1.best_estimator_, \n",
    "                        search_spaces= param_test3,\n",
    "                      scoring=avg_prec,\n",
    "                      n_iter=5)\n",
    "\n",
    "\n",
    "xgb_bo2.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.73      0.82        98\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56962\n",
      "   macro avg       0.97      0.87      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "\n",
      "[[56859     5]\n",
      " [   26    72]]\n"
     ]
    }
   ],
   "source": [
    "xgb_bo2.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "preds_bo2 = xgb_bo2.best_estimator_.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds_bo2))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_bo2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000713554909452796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56864\n",
      "           1       0.03      0.94      0.05        98\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56962\n",
      "   macro avg       0.51      0.94      0.51     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n",
      "\n",
      "\n",
      "[[53379  3485]\n",
      " [    6    92]]\n"
     ]
    }
   ],
   "source": [
    "thres_bo2 = m.optimal_cutoff(y_test, xgb_bo2.best_estimator_.predict_proba(X_test)[:,1])[0]\n",
    "print(thres_bo2)\n",
    "preds_new2 = [x > thres_bo2 for x in xgb_bo2.best_estimator_.predict_proba(X_test)[:,1]]\n",
    "print(classification_report(y_test, preds_new2))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_new2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\wband\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0.0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=2, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=True, n_iter=5, n_jobs=1, n_points=1,\n",
       "       optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "       refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "       search_spaces={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]}, verbose=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "xgb_bo3 = BayesSearchCV(estimator=xgb_bo2.best_estimator_, \n",
    "                        search_spaces= param_test3,\n",
    "                      scoring=avg_prec,\n",
    "                      n_iter=5)\n",
    "\n",
    "\n",
    "xgb_bo3.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.73      0.82        98\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56962\n",
      "   macro avg       0.97      0.87      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "\n",
      "[[56859     5]\n",
      " [   26    72]]\n"
     ]
    }
   ],
   "source": [
    "xgb_bo3.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "preds_bo3 = xgb_bo3.best_estimator_.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds_bo3))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_bo3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000713554909452796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     56864\n",
      "           1       0.03      0.93      0.05        98\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56962\n",
      "   macro avg       0.51      0.93      0.51     56962\n",
      "weighted avg       1.00      0.94      0.97     56962\n",
      "\n",
      "\n",
      "\n",
      "[[53357  3507]\n",
      " [    7    91]]\n"
     ]
    }
   ],
   "source": [
    "thres_bo3 = m.optimal_cutoff(y_test, xgb_bo3.best_estimator_.predict_proba(X_test)[:,1])[0]\n",
    "print(thres_bo2)\n",
    "preds_new3 = [x > thres_bo2 for x in xgb_bo3.best_estimator_.predict_proba(X_test)[:,1]]\n",
    "print(classification_report(y_test, preds_new3))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds_new3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
